---
title: "Spam or Ham message Classification"
author: "Anish Singh Walia"
output:
  html_document: default
  html_notebook: default
---


Requiring the necessary packages-
```{r,message=FALSE,warning=FALSE}

require(quanteda)#natural language processing package
?quanteda 
require(RColorBrewer)
require(ggplot2)

```

__quanteda__ makes it easy to manage texts in the form of a __corpus__, defined as a collection of texts that includes document-level variables specific to each text, as well as meta-data for documents and for the collection as a whole. quanteda includes tools to make it easy and fast to manuipulate the texts in a corpus, by performing the most common natural language processing tasks simply and quickly, such as tokenizing, stemming, or forming ngrams. quantedaâ€™s functions for tokenizing texts and forming multiple tokenized documents into a document-feature matrix are both extremely fast and extremely simple to use. quanteda can segment texts easily by words, paragraphs, sentences, or even user-supplied delimiters and tags.

--------------


####loading the dataset


```{r}
spam<-read.csv("F:/PROJECTS/Datasets/spam.csv",header=TRUE, sep=",", quote='\"\"', stringsAsFactors=FALSE)

table(spam$v1)


#checking the distribution of type of messages
theme_set(theme_bw())
ggplot(aes(x=v1),data=spam) +
  geom_bar(fill="red",width=0.5)



```

Now let's add appropiate names to the columns.


```{r}
names(spam)<-c("type","message")
head(spam)


```

Now we can sample the data.We can randomize our data using the sample() command.If the data is not stored in a random distribution, this will help to ensure that we are dealing with a random draw from our data. The set.seed() is to ensure reproducable results.

```{r}
set.seed(2012)
spam<-spam[sample(nrow(spam)),]

```


------------------


###Now let's build the spam and ham Wordclouds

We'll use quanteda's corpus() command to construct a corpus from the Text field of our raw data.A corpus can be thought of as a master copy of our dataset from which we can pull subsets or observations as needed.

After this I will attach the Label field as a document variable to the corpus using the docvars() command. We attach Label as a variable directly to our corpus so that we can associate SMS messages with their respective ham/spam label later in the analysis.

```{r}

?corpus #to search more on this method
msg.corpus<-corpus(spam$message)
docvars(msg.corpus)<-spam$type   #ataching the label to the corpus message text

```

Let's plot the wordcloud now-

```{r,fig.height=6,fig.width=8,out.width="80%"}
#subsetting only the spam messages
spam.plot<-corpus_subset(msg.corpus,docvar1=="spam")

#now creating a document-feature matrix using dfm()
spam.plot<-dfm(spam.plot, tolower = TRUE, remove_punct = TRUE, remove_twitter = TRUE, remove_numbers = TRUE, remove=stopwords("SMART"))

spam.col <- brewer.pal(10, "BrBG")  

textplot_wordcloud(spam.plot, min.freq = 16, color = spam.col)  
title("Spam Wordcloud", col.main = "grey14")

```



------------


###Generating the Ham wordcloud

```{r,fig.height=6,fig.width=8,out.width = '80%'}
ham.plot<-corpus_subset(msg.corpus,docvar1=="ham")
ham.plot<-dfm(ham.plot,tolower = TRUE, remove_punct = TRUE, remove_twitter = TRUE, remove_numbers = TRUE,remove=c("gt", "lt", stopwords("SMART")))
ham.col=brewer.pal(10, "BrBG")  
textplot_wordcloud(ham.plot,min.freq=50,colors=ham.col,fixed.asp=TRUE)
title("Ham Wordcloud",col.main = "grey14")

```

-------------


##Prediction Using Naive Bayes Classifier

Naive Bayes classifiers are a class of simple linear classifiers which  are *conditional probability* models based on __Bayes__ Theoram i.e 
$$P(Y \in K_j | X_i) = P(X_1|Y).P(X_2|Y)......P(X_i|Y) . \ P(Y \in K_j)$$ 
$where \  X_i \ \text{are the number of inputs and Y is discrete response variable and } K_j \ \text{are the number of class labels}$.

The special thing about Naive Bayes classifiers are that they follow _Conditional Independence Theoram_ i.e the features $X_i$ are uncorrelated and independent of each other which is often always crude.Secondly,they assume that the data samples are drawn from a identical and independent distribution- __IID__ is the term which is famous in Statistics.


```{r,warning=FALSE,message=FALSE}
#separating Train and test data
spam.train<-spam[1:4458,]
spam.test<-spam[4458:nrow(spam),]

msg.dfm <- dfm(msg.corpus, tolower = TRUE)  #generating document freq matrix
msg.dfm <- dfm_trim(msg.dfm, min_count = 5, min_docfreq = 3)  
msg.dfm <- dfm_weight(msg.dfm, type = "tfidf") 

#trining and testing data of dfm 
msg.dfm.train<-msg.dfm[1:4458,]

msg.dfm.test<-msg.dfm[4458:nrow(spam),]


```

Training the Naive Bayes classifier-

```{r}

nb.classifier<-textmodel_NB(msg.dfm.train,spam.train[,1])
nb.classifier



```

The model outputs the Probabilities of the message being Spam or ham.


------------

###Let's Test the Model 

```{r}

pred<-predict(nb.classifier,msg.dfm.test)

#generating a confusion matrix

# use pred$nb.predicted to extract the class labels
table(predicted=pred$nb.predicted,actual=spam.test[,1])
#16 wrongly classified for ham and 7 examples wrongly classified for spam

#acccuracy of the classifier on Test data
mean(pred$nb.predicted==spam.test[,1])*100
#accuracy of 97% on test set

```
In the Confusion matrix , the __off-diagonals__ are the correctly classified examples. 



----------

##Conclusion


This was a simple project on Classifying text messages as __ham__ or __spam__ using some natural language proecessing and then building a Naive Bayes text classifier.Ofcourse,there are various other packages to do text processing and building such models.

Hope you guys liked the article , make sure to like and share it.

