---
title: "Spam or Ham message Classification(PART 2)"
author: "Anish Singh Walia"
output:
  html_notebook: default
  html_document: default
---


Now in the previous article on classifying the text messages as Sapm or ham we observed that the class lebels were very much biased towards 'Ham' messages and were unbalanced.So to avoid this situation and avoid the Classifier to interpret wrong information and learn the Bias from the data we could either go for a __undersampling__ technique which will under-sample the majority class and try to balance the dataset whereas there is another technique of __oversampling__ i,e increasing the proportion of minority class and balance the data.

We will use over-sampling technique.A famous and well know over-sampling technique is __SMOTE__. SMOTE(Chawla et. al. 2002)  is termed as __Synthetic Minority Oversampling technique__. The general idea of this method is to artificially generate new examples of the __minority__ class using the K-NN algorithm i.e finding the __k__ nearest neighbor of these cases. Furthermore, the majority class examples are also *under-sampled*, leading to a more balanced dataset.

__SMOTE is most efficient and best for low dimentional problems where $p < n$ i.e number of predictors are less.__

Now due to balanced classes in the training dataset our classifer can learn in a more better and efficient manner without learning the bias in the classes earlier. Now if a learning algorithm also learns the bias towards a particular class ,then that could actually cause problems and degraded performance in Testing the particular Model on random unseen text cases.

I urge readers to go through a very amazing article explaining the merits and demerits of SMOTE-https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-106.
